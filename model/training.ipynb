{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3Ecuf1s7AG1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJBHNX90mKed",
        "outputId": "bbe27a0f-1a0b-4701-cc73-3646f0099740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "fea3edbb9a244c3cb37fb2494ac94a1c",
            "cd9e9ecadbfa4f05bc61026e1c6417ea",
            "4893b00676a94fb382286acbc301b31f",
            "aed7bbec4a5c42a689a97a2abd359709",
            "35251cf323a0446fa788b6210d6be724",
            "03d59e7d3f8b4374a109b6875522f9f8",
            "c09f72a3e9274d5d8226f4b4915e6473",
            "473e37386941489f9615bdfe41a00100",
            "5ff1cbdb4e54437891534cd4d421b0c9",
            "276244f7eee94360a0461908cf55f21e",
            "9042c48cacec4b808a966d36094f4eea",
            "87e52a47d3124f9b8225ca6146cc403b",
            "58ee54081f0247c9b268bfc2caa784af",
            "2ab53a21b3124bfcbf998732b0955984",
            "b901ad5386c14bc9aa83f02a59c5c5a5",
            "2749b1b7dd7a4b6e9debaed5c8ba54bb",
            "4a83df9ea39740348a89524cb998a685",
            "c834e751fef644ddbb4cbbb01c3b9e50",
            "5d9b134ccf524525abf0c511f4cb0847",
            "b41b013487c641769198959bd4b033ed",
            "86603a1c3d4b4152b3491296618d88c0",
            "e47cbb9e98f54b91926196081344fc9a",
            "5906cfac30c547aa9453dadf9ca64a16",
            "d99cdc2360c04e25927d6ec11d3c0b85",
            "4ea9b7f0cd4f4476b538ab57c217349d",
            "e8aeeff8bbfd41c483ee0e9b3ada4047",
            "d6a8044a80344ea98c27f8261776d676",
            "9cc4c600805148d8a4c0b96801e7f25f",
            "02656990eb7f4182be49cad863f04d29",
            "5f9bc134e76e4e1296356a99a4317dca",
            "bdcab4a628e345179651df739ca2435b",
            "def248f9f0c44c57abb585281b23e194"
          ]
        },
        "id": "6_OcGkzanyIE",
        "outputId": "40a71f87-7ae7-4b2c-ec8f-71b504d38973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fea3edbb9a244c3cb37fb2494ac94a1c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # reduce the amount of console output from TF\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install sentencepiece\n",
        "from transformers import *\n",
        "!pip install -q datasets # install HF datasets library\n",
        "from datasets import load_dataset\n",
        "\n",
        "logging.set_verbosity_warning()\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import logging\n",
        "\n",
        "print('TF version',tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) # check GPU available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNP2Vlovmgn9",
        "outputId": "2cdd6741-da59-417f-e105-115bf6c77478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTF version 2.14.0\n",
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_strategy(xla, fp16, no_cuda):\n",
        "    print(\" Tensorflow: setting up strategy\")\n",
        "\n",
        "    # setup xla\n",
        "    if xla:\n",
        "        print(\" XLA Enabled\")\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "\n",
        "    # setup mixed precision training\n",
        "    if fp16:\n",
        "        # Set to float16 at first\n",
        "        print(\" Mixed Precision Training Enabled\")\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "\n",
        "    # setup distribution strategy\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if no_cuda:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    else:\n",
        "        if len(gpus) == 0:\n",
        "            print(\" One Device Strategy [CPU] Enabled\")\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "        elif len(gpus) == 1:\n",
        "            print(\" One Device Strategy [GPU] Enabled\")\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "        elif len(gpus) > 1:\n",
        "            print(\" Mirrored Strategy Enabled\")\n",
        "            # If only want to use a specific subset of GPUs use CUDA_VISIBLE_DEVICES=0`\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return strategy\n",
        "\n",
        "def n_replicas(strategy):\n",
        "    # return number of devices\n",
        "    return strategy.num_replicas_in_sync\n",
        "\n",
        "# note:\n",
        "# huggingface TF-T5 implementation has issues when mixed precision is enabled\n",
        "# we will disable FP16 for this but can be used for training any other model\n",
        "strategy = setup_strategy(xla=True, fp16=False, no_cuda=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LszbzBRim0yN",
        "outputId": "853eb847-74bd-4c87-b51f-2f13169d66fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tensorflow: setting up strategy\n",
            " XLA Enabled\n",
            " One Device Strategy [GPU] Enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(cache_dir):\n",
        "    # download data using a keras utility\n",
        "    _url = \"https://raw.githubusercontent.com/Goutami-Sooda/ITRL-Project/main/Dataset(JSON).json\" # download mbpp dataset\n",
        "    dataset_path = tf.keras.utils.get_file(\"mbpp.jsonl\", origin=_url, cache_dir=cache_dir, cache_subdir=cache_dir)\n",
        "    return dataset_path\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args):\n",
        "    # encode text-code pairs\n",
        "    texts = examples['text']\n",
        "    codes = examples['code']\n",
        "    # tests = [\" \".join(test) for test in examples['test_list']] # convert list of test cases to single string\n",
        "\n",
        "    # encode texts by prepending the task for input sequence\n",
        "    inputs = [args.prefix + text for text in texts]\n",
        "    model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # encode texts by prepending the task for input sequence and appending the test sequence\n",
        "    # inputs = [args.prefix + text + \" \" + test for text, test in zip(texts, tests)]\n",
        "    # model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # encode texts by prepending the task for input sequence\n",
        "    labels = tokenizer(codes, max_length=args.max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "    # we need to replace the index of the padding tokens by -100\n",
        "    # such that they are not taken into account by the CrossEntropyLoss\n",
        "    labels_with_ignore_index = []\n",
        "    for labels_example in labels:\n",
        "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
        "        labels_with_ignore_index.append(labels_example)\n",
        "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
        "\n",
        "    # return features\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def get_train_tfdataset(train_dataset, num_train_examples, args):\n",
        "    # select feature columns\n",
        "    columns = ['input_ids', 'attention_mask', 'labels']\n",
        "    # set to tensorflow format\n",
        "    train_dataset.set_format(type='tensorflow', columns=columns)\n",
        "\n",
        "    # specify return types\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n",
        "    # specify return shapes\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n",
        "    # initialize dataset\n",
        "    tf_dataset = tf.data.Dataset.from_generator(lambda : train_dataset, return_types, return_shapes)\n",
        "\n",
        "    # turn off auto-sharding\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "    tf_dataset = tf_dataset.with_options(options)\n",
        "\n",
        "    # repeat, shuffle, batch, prefetch\n",
        "    ds = (\n",
        "        tf_dataset.repeat()\n",
        "        .shuffle(num_train_examples, seed=args.seed)\n",
        "        .batch(args.train_batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # distribute dataset to devices\n",
        "    return strategy.experimental_distribute_dataset(ds)\n",
        "\n",
        "def get_validation_tfdataset(eval_dataset, num_validation_examples, args):\n",
        "    # select feature columns\n",
        "    columns = ['input_ids', 'attention_mask', 'labels']\n",
        "    # set to tensorflow format\n",
        "    eval_dataset.set_format(type='tensorflow', columns=columns)\n",
        "\n",
        "    # specify return types\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n",
        "    # specify return shapes\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n",
        "    # initialize dataset\n",
        "    tf_dataset = tf.data.Dataset.from_generator(lambda : eval_dataset, return_types, return_shapes)\n",
        "\n",
        "    # turn off auto-sharding\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "    tf_dataset = tf_dataset.with_options(options)\n",
        "\n",
        "    # repeat, batch, prefetch\n",
        "    ds = (\n",
        "        tf_dataset.repeat()\n",
        "        .batch(args.validation_batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # distribute dataset to devices\n",
        "    return strategy.experimental_distribute_dataset(ds)"
      ],
      "metadata": {
        "id": "MrOeBr17nPUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_all_seeds(seed):\n",
        "    # set random seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def init_logger(log_file=None, log_file_level=logging.NOTSET):\n",
        "    # initialize logger for tracking events and save in file\n",
        "    if isinstance(log_file, Path):\n",
        "        log_file = str(log_file)\n",
        "    log_format = logging.Formatter(\n",
        "        fmt='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "        datefmt='%m/%d/%Y %H:%M:%S'\n",
        "    )\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(log_format)\n",
        "    logger.handlers = [console_handler]\n",
        "    if log_file and log_file != '':\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setLevel(log_file_level)\n",
        "        # file_handler.setFormatter(log_format)\n",
        "        logger.addHandler(file_handler)\n",
        "    return logger\n",
        "\n",
        "class ProgressBar(object):\n",
        "    # custom progress bar\n",
        "    def __init__(self, n_total,width=30,desc = 'Training'):\n",
        "        self.width = width\n",
        "        self.n_total = n_total\n",
        "        self.start_time = time.time()\n",
        "        self.desc = desc\n",
        "\n",
        "    def __call__(self, step, info={}):\n",
        "        now = time.time()\n",
        "        current = step + 1\n",
        "        recv_per = current / self.n_total\n",
        "        bar = f'[{self.desc}] {current}/{self.n_total} ['\n",
        "        if recv_per >= 1:\n",
        "            recv_per = 1\n",
        "        prog_width = int(self.width * recv_per)\n",
        "        if prog_width > 0:\n",
        "            bar += '=' * (prog_width - 1)\n",
        "            if current< self.n_total:\n",
        "                bar += \">\"\n",
        "            else:\n",
        "                bar += '='\n",
        "        bar += '.' * (self.width - prog_width)\n",
        "        bar += ']'\n",
        "        show_bar = f\"\\r{bar}\"\n",
        "        time_per_unit = (now - self.start_time) / current\n",
        "        if current < self.n_total:\n",
        "            eta = time_per_unit * (self.n_total - current)\n",
        "            if eta > 3600:\n",
        "                eta_format = ('%d:%02d:%02d' %\n",
        "                              (eta // 3600, (eta % 3600) // 60, eta % 60))\n",
        "            elif eta > 60:\n",
        "                eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
        "            else:\n",
        "                eta_format = '%ds' % eta\n",
        "            time_info = f' - ETA: {eta_format}'\n",
        "        else:\n",
        "            if time_per_unit >= 1:\n",
        "                time_info = f' {time_per_unit:.1f}s/step'\n",
        "            elif time_per_unit >= 1e-3:\n",
        "                time_info = f' {time_per_unit * 1e3:.1f}ms/step'\n",
        "            else:\n",
        "                time_info = f' {time_per_unit * 1e6:.1f}us/step'\n",
        "\n",
        "        show_bar += time_info\n",
        "        if len(info) != 0:\n",
        "            show_info = f'{show_bar} ' + \\\n",
        "                        \"-\".join([f' {key}: {value:.4f} ' if key != \"learning_rate\" else f' {key}: {value:.8f} ' for key, value in info.items()])\n",
        "            print(show_info, end='')\n",
        "        else:\n",
        "            print(show_bar, end='')"
      ],
      "metadata": {
        "id": "6iXrh1hbnUpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, args, train_dataset, validation_dataset,\n",
        "        num_train_examples, num_validation_examples, push_to_hub\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "        self.train_dataset = train_dataset\n",
        "        self.num_train_examples = num_train_examples\n",
        "\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.num_validation_examples = num_validation_examples\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.eval_loss = tf.keras.metrics.Sum()\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
        "        # creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.\n",
        "        num_warmup_steps = math.ceil(num_training_steps * self.args.warmup_ratio)\n",
        "        self.optimizer, self.lr_scheduler = create_optimizer(\n",
        "            init_lr=self.args.learning_rate,\n",
        "            num_train_steps=num_training_steps,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            weight_decay_rate=self.args.weight_decay,\n",
        "            adam_epsilon=self.args.adam_epsilon\n",
        "        )\n",
        "\n",
        "    def evaluation_step(self, features, labels, nb_instances_in_global_batch):\n",
        "        # forward pass\n",
        "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=False)[:2]\n",
        "        loss, logits = outputs[:2]\n",
        "        # loss scaling\n",
        "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
        "        # add current batch loss\n",
        "        self.eval_loss.update_state(scaled_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_evaluation_steps(self, batch):\n",
        "        features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
        "        labels = batch['labels']\n",
        "        nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
        "        # strategy.run() expects args to be a list or tuple\n",
        "        inputs = (features, labels, nb_instances)\n",
        "        # `run` replicates the provided computation and runs with the distributed input\n",
        "        strategy.run(self.evaluation_step, inputs)\n",
        "\n",
        "    def evaluate(self):\n",
        "        # calculate total validation steps\n",
        "        steps = math.ceil(self.num_validation_examples / self.args.validation_batch_size)\n",
        "        # reset eval loss after every epoch\n",
        "        self.eval_loss.reset_states()\n",
        "        logs = {}\n",
        "        pbar = ProgressBar(n_total=steps, desc='Evaluating')\n",
        "        # iterate over validation dataset\n",
        "        for step, batch in enumerate(self.validation_dataset):\n",
        "            # distributed evaluation step\n",
        "            self.distributed_evaluation_steps(batch)\n",
        "            logs[\"eval_loss\"] = self.eval_loss.result() / (step + 1)\n",
        "            pbar(step=step, info=logs)\n",
        "            if step == steps - 1:\n",
        "                break\n",
        "        print(\"\\n------------- validation result -----------------\")\n",
        "\n",
        "    def apply_gradients(self, features, labels, nb_instances_in_global_batch):\n",
        "        # forward pass\n",
        "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=True)[:2]\n",
        "        loss, logits = outputs[:2]\n",
        "        # loss scaling\n",
        "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
        "        # calculate gradients\n",
        "        gradients = tf.gradients(scaled_loss, self.model.trainable_variables)\n",
        "        # convert gradients with nan value\n",
        "        gradients = [g if g is not None else tf.zeros_like(v) for g, v in zip(gradients, self.model.trainable_variables)]\n",
        "        # optimize the model\n",
        "        self.optimizer.apply_gradients(list(zip(gradients, self.model.trainable_variables)))\n",
        "        # add current batch loss\n",
        "        self.train_loss.update_state(scaled_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_training_steps(self, batch):\n",
        "        with strategy.scope():\n",
        "            features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
        "            labels = batch['labels']\n",
        "            nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
        "            # strategy.run() expects args to be a list or tuple\n",
        "            inputs = (features, labels, nb_instances)\n",
        "            # `run` replicates the provided computation and runs with the distributed input.\n",
        "            strategy.run(self.apply_gradients, inputs)\n",
        "\n",
        "    def train(self):\n",
        "        # calculate total training steps\n",
        "        num_updates_per_epoch = self.num_train_examples // args.train_batch_size\n",
        "        self.steps_per_epoch = num_updates_per_epoch\n",
        "        t_total = self.steps_per_epoch * self.args.epochs\n",
        "\n",
        "        with strategy.scope():\n",
        "            # optimizer, and checkpoint must be created under `strategy.scope`\n",
        "            # create optimizer and scheduler\n",
        "            self.create_optimizer_and_scheduler(num_training_steps=t_total)\n",
        "\n",
        "            # create checkpoint manager\n",
        "            folder = os.path.join(self.args.output_dir, self.args.checkpoint_dir)\n",
        "            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)\n",
        "            self.model.ckpt_manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
        "            iterations = self.optimizer.iterations\n",
        "\n",
        "            logger.info(\"***** Running training *****\")\n",
        "            logger.info(f\"  Num examples = {self.num_train_examples}\")\n",
        "            logger.info(f\"  Num Epochs = {self.args.epochs}\")\n",
        "            logger.info(f\"  Total train batch size (w. parallel & distributed) = {self.args.train_batch_size * n_replicas(strategy)}\")\n",
        "            logger.info(f\"  Steps per epoch = {self.steps_per_epoch}\")\n",
        "            logger.info(f\"  Total optimization steps = {t_total}\")\n",
        "\n",
        "            self.train_loss = tf.keras.metrics.Sum(name=\"training_loss\")\n",
        "            start_time = datetime.datetime.now()\n",
        "            for epoch_iter in range(self.args.epochs):\n",
        "                # training loop\n",
        "                logger.info(f\"Epoch {epoch_iter + 1}/{self.args.epochs}\")\n",
        "\n",
        "                pbar = ProgressBar(n_total=self.steps_per_epoch, desc='Training')\n",
        "                # iterate over training dataset\n",
        "                for step, batch in enumerate(self.train_dataset):\n",
        "                    # distributed training step\n",
        "                    self.distributed_training_steps(batch)\n",
        "\n",
        "                    self.global_step = iterations.numpy()\n",
        "                    training_loss = self.train_loss.result() / (step + 1)\n",
        "\n",
        "                    logs = {}\n",
        "                    logs[\"training_loss\"] = training_loss.numpy()\n",
        "                    logs[\"learning_rate\"] = self.lr_scheduler(self.global_step).numpy()\n",
        "                    pbar(step=step, info=logs)\n",
        "\n",
        "                    if self.global_step % self.steps_per_epoch == 0:\n",
        "                        print(\"\\n------------- train result -----------------\")\n",
        "                        # call to evaluation loop\n",
        "                        self.evaluate()\n",
        "                        # save checkpoint\n",
        "                        ckpt_save_path = self.model.ckpt_manager.save()\n",
        "                        logger.info(f\"Saving checkpoint at {ckpt_save_path}\")\n",
        "                        break\n",
        "\n",
        "                # reset train loss after every epoch\n",
        "                self.train_loss.reset_states()\n",
        "            end_time = datetime.datetime.now()\n",
        "            logger.info(f\"Training took: {str(end_time - start_time)}\")"
      ],
      "metadata": {
        "id": "Ds6kXnZvnXCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(args):\n",
        "    logger.info(\" Starting training / evaluation\")\n",
        "\n",
        "    logger.info(\" Downloading Data Files\")\n",
        "    dataset_path = download_dataset(args.cache_dir)\n",
        "\n",
        "    logger.info(\" Loading Data Files\")\n",
        "    dataset = load_dataset('json', data_files=dataset_path)\n",
        "    # train test split\n",
        "    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n",
        "\n",
        "    logger.info(\" Initializing Tokenizer\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n",
        "\n",
        "    logger.info(\" Preparing Features\")\n",
        "    dataset = dataset.map(convert_examples_to_features, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"args\":args})\n",
        "\n",
        "    logger.info(\" Intializing training and validation dataset \")\n",
        "    train_dataset = dataset['train']\n",
        "    num_train_examples = len(dataset['train'])\n",
        "    # create tf train dataset\n",
        "    tf_train_dataset = get_train_tfdataset(train_dataset, num_train_examples, args)\n",
        "\n",
        "    validation_dataset = dataset['test']\n",
        "    num_validation_examples = len(dataset['test'])\n",
        "    # create tf validation dataset\n",
        "    tf_validation_dataset = get_validation_tfdataset(train_dataset, num_validation_examples, args)\n",
        "\n",
        "    logger.info(f' Intializing model | {args.model_type.upper()} ')\n",
        "    with strategy.scope():\n",
        "        # model must be created under `strategy.scope`\n",
        "        model = TFT5ForConditionalGeneration.from_pretrained(args.model_name_or_path, from_pt=True)\n",
        "\n",
        "    # custom training loop\n",
        "    trainer = Trainer(model, args, tf_train_dataset, tf_validation_dataset, num_train_examples, num_validation_examples,push_to_hub = True)\n",
        "    trainer.train()\n",
        "\n",
        "    # save pretrained model and tokenizer\n",
        "    logger.info(f\" Saving model in {args.save_dir}\")\n",
        "    trainer.model.save_pretrained(args.save_dir)\n",
        "    trainer.model.push_to_hub(\"AshArya/ITRLTrained\")\n",
        "    tokenizer.save_pretrained(args.save_dir)\n",
        "    tokenizer.push_to_hub(\"AshArya/ITRLTrained\")"
      ],
      "metadata": {
        "id": "h307QJ1unY9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    # define training arguments\n",
        "\n",
        "    # MODEL\n",
        "    model_type = 't5'\n",
        "    tokenizer_name = 'Salesforce/codet5-base'\n",
        "    model_name_or_path = 'Salesforce/codet5-base'\n",
        "\n",
        "    # DATA\n",
        "    train_batch_size = 8\n",
        "    validation_batch_size = 8\n",
        "    max_input_length = 48\n",
        "    max_target_length = 128\n",
        "    prefix = \"Generate Python: \"\n",
        "\n",
        "    # OPTIMIZER\n",
        "    learning_rate = 3e-4\n",
        "    weight_decay = 1e-4\n",
        "    warmup_ratio = 0.2\n",
        "    adam_epsilon = 1e-8\n",
        "\n",
        "    # TRAINING\n",
        "    seed = 2022\n",
        "    epochs = 20\n",
        "\n",
        "    # DIRECTORIES\n",
        "    output_dir = \"runs/\"\n",
        "    logging_dir = f\"{output_dir}/logs/\"\n",
        "    checkpoint_dir = f\"checkpoint\"\n",
        "    save_dir = f\"{output_dir}/saved_model/\"\n",
        "    cache_dir = '../working/'\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# initialize training arguments\n",
        "args = Args()\n",
        "# initialize logger\n",
        "logger = init_logger(log_file=os.path.join(args.logging_dir, f\"{args.model_type}-{time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())}.log\"))\n",
        "# fix all seeds\n",
        "fix_all_seeds(args.seed)\n"
      ],
      "metadata": {
        "id": "SZiVhhCNnbyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # run training and evaluation\n",
        "    dataset = run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29a134935e5843df87bed26b2adc16e0",
            "22587766e3dc4cc7adbf8703f0384a15",
            "1f082130cce3412098eed510039adabb",
            "d73d035d1c344b039b1b2f19afa291b9",
            "6c5b530f739e47aba7d6b56d66644470",
            "06eeb495adb845bba62606d3ca0f7464",
            "bc58a621b5a24eedb7580a65a3c290ee",
            "9c65509086564edb83f501ce159aa5c9",
            "a43ee72d99c54d0291372e661f6ca60b",
            "0678608849f044f796a44b758cc18256",
            "66e9ac3c723e48f8a261ce86119b2bdb",
            "fc16b8c2b7cf46eab086418c50db4e98",
            "0d1f58d0c6cc4eb0ad1cacc9c971cc2f",
            "587b63698caa4c6392f261f87b155937",
            "65962555639542bd9d0631ce05720480",
            "d77ea56c11c1452e8fe051feb1cbf382",
            "90951670a5504566bda29ba73c0a2cd5",
            "8485d53a3bd34c6482d2fccbd186c4f1",
            "0d8d1ee82715492f9b27e772a1855d27",
            "0eebc943fb33440da61f88b1fc4195c3",
            "e268fc0fa1ac4a36961366d47a46ee41",
            "21bde369b858472e9f019cfeea7e9912",
            "a6a600d47f084d2d8beefb6749dc8269",
            "c4d176a307a34f2eb110523a17f3c1e1",
            "983cbd00bc1d4d1ab33400739e783352",
            "194ec70ebcf14559bf6b46e5a65bf912",
            "e04896a251f94177ade71e8668d3b2c6",
            "93a3660fb87e458e97511acbae83d241",
            "d0ff4bb5607b4d238bbf247e719be86b",
            "4b4f06d0f07a4aa19abc660475ec9cc8",
            "fcec8d733a8e46d6be61e95c81dc69e7",
            "22d1ec71c682496eaa8af73b1794317f",
            "6c2fb2f38ccc49a189eb0de4e1851551",
            "5a781f68e8234babb9423cda4b5a4e89",
            "2c235f72dba344b7a73b7d5e38134d17",
            "71c3fb49f1eb430cb2a79cbdb8383cd0",
            "d468886601a54f1e9451065257b61db7",
            "7ead65e230cf4b97b225ad9c6bdecb58",
            "62dd5b34bad1441698e44f058d98e69a",
            "d66071b5c90848fea0ccb81f53a15d0c",
            "bf174c97b77145afbbaf6a30024fe513",
            "8ac2a49116354d159744a6b73dad62d9",
            "fbe53636049b4b33a50ba295a44ae49f",
            "5bc77500f38345c6842fd11625ebcf24",
            "8c39ad8f25044fc4b44a2f17423516c3",
            "cbf418df904443fe9232ac28d6bd3543",
            "9c166fe6a3c346568bbceea836c785aa",
            "3c08339153a64afea6cb70211820faef",
            "0f707654c9754002a05671d6fa5ae223",
            "670c4a33855e4496b220c3eadcc85b1b",
            "cf59774a2bbf44cf85122a65b38d6eca",
            "1afa201414d24f3985a0a026d1309f71",
            "4b65d305154f4914a1cf98a7e74cbd4e",
            "6a19d116dde14540a1d8db409c11195e",
            "053e76cb3be14fdb8333dec78d7cb9b5",
            "aaf6bf40c9204486a56748685882cd6f",
            "bbedc91bddd84cfeaea21bb0b66e7d51",
            "57d8bfdb2a4e4f3ab01fe2ce4808e6cd",
            "7d8e5661ab82445eaa42291226bab0c0",
            "6eb69fc92c3842999a53ea003225df51",
            "562893733f48476a93fac2d31af1b53d",
            "1746f555359c4dce9938efa7f1dda435",
            "6ff8e2f9a02d4df4b4b33d51100588ba",
            "f5724ed82970414d8275c3aa732efd5b",
            "69f9384726ed43608ab4ed6e072fa9c7",
            "1a9f7caae01c4b3da06203a085f645ca",
            "fab71ebe959745b2b64da2ad444229a9",
            "c13d1d99ecde46c0825ff52bcb8a135f",
            "cbd2348ab0594c44a25d883a546842de",
            "16435c82e5994f5783b192e8f5a6bcf5",
            "fc8b20d8f07240b1814b0d9d125e835e",
            "27b3a534f8f14c80855f2a2455c5d520",
            "bef811cee2984d878dbe1dfabb557e3f",
            "e3e5a765b4cd49e3b8685dd9033d608b",
            "e64c1d2d3448491dbaf072cbc395b874",
            "2f8d55fc97674b2a8454292c7c5387b8",
            "277c9739cf7f454dbf280ca7f85daa30",
            "a02d7710b3f6408d9bdd4010e37a2900",
            "61cca0a769324e0ab34aebe4ad71383f",
            "a34cf045f4e3463e93734a4c549e4b54",
            "84d07bba7a704061a9f5ef57690d22c5",
            "28a0d34fb99345ccb97b5307131e8346",
            "154bcfb5b8064a86879aae601036b931",
            "c88873c6c0ee499eb48f4aee816d494a",
            "9696097bf919408c929683ded40aa091",
            "63e50ea6673249a983ec0ed1838f12a4",
            "74863f5da7984900b861aac61ec51416",
            "0646d145673c4e75be97d8d04c39ea1f",
            "8486958ada0442e2850f1ae3673f88c6",
            "3b29de53af214d538223158cadd2964a",
            "00ceb347412f428ba6957f9b726ee360",
            "f8b5597a9b904e5485875271824fe611",
            "d048c0cdf6af4210ae01fee7d0058de5",
            "6b62df8995b84a94857db6498bac7d85",
            "942e85e7a65b47f0b9f59de2f7cde1cd",
            "a728a05b27a648fb9c452304408bb8b6",
            "1f759b3b5cd3489290394446dc636886",
            "47df0e1a3d414fce8c048f1094aa1eb4",
            "06d79ed670bf4458b6445dc59a4b18f8",
            "d60a1ed2f9a942179064a1018ff7c1f6",
            "30e7a6749dbf4994be2c18e8e2466579",
            "57c53cd58f3f468589394830baeb52b2",
            "4d4a27638cf7463ea0dba71847ebf52b",
            "29717ca537cb4918917b076d05284a97",
            "fdd7009dd78544f68e3d5d5f9a1df367",
            "e5e6863572384e7d9dcc3b4f26b7be08",
            "de33c247e20e4e55b5babed3b8a59444",
            "e19d6e1b09e641b0b79065d372c9b78a",
            "c1fd506965374157937d1fe5092a6a45",
            "5a64141e7f2941e5b0de4b2ccc7f8925",
            "c1001db3b11944d18af0d4508e12f51e",
            "df79047314b847189512758772017565",
            "952ce5c1f557466cbc14f723e6412cef",
            "53d37061304d4d2db3d15cee4bab4dea",
            "97bf96c4d5704c09828826a2e5dee683",
            "35d4e3bcbf6c4ea6b44bfd828531a794",
            "1b05c785412a4bbcb4918e86943f28e0",
            "44717fff245f4f889a41f1fdae864f92",
            "94e70ef7037e4fd099036cecb9ae7794",
            "fe129d93088a485bbbfa90654fe44cec",
            "c63c4c7e9afa4e3a96f6dc213b297dce",
            "8c303181723540b58d66312e76a92fb5",
            "95d652e32f714bb1ad0847cb42aac6d1",
            "a7b0762f43d6442e9ed4df0aa6dbbf13",
            "75835dc8091a4d1c9ebd7d42b5c8bdf4",
            "7441089e4eb945a9816f30efce2fb0c2",
            "70f88b0aa4ba49bd80eaf7b46330a44d",
            "d27ee0f68c6144fbbf0c817e6a87626a",
            "2a918ad6e8bc4794baabc4aac784cc01",
            "40529b0588fa49ac934913574e10d333",
            "1ce7b0e342e34a5eb49a028146af9bfe",
            "1ba01316c10f4f27a96ac90ac4fb66c9",
            "0868feda852a4d31b128188eee7b839f",
            "e7886ec9568241a3be161dc905e19fd0",
            "5bbd632f9b384b1094740204d9ec3d4b",
            "a0ea3893384b4c0da230efeff10bb1cf",
            "3f767d7eb81c4ee6adb090be07dee1d8",
            "c96506e7f5314d12a1197502425633d3",
            "e4de5ac7622b4208a45cda13064fc643",
            "41eb60913c904b7ba7d1aa286aa7910f",
            "6372ad60bc0443a49e7dba34b7591d47",
            "9702641bd1c748279ea3142b56d7894d",
            "9e6184d18bea4363840c92b2c8ee5cc3"
          ]
        },
        "id": "7DW84091oWsu",
        "outputId": "20d06029-bbd5-49ea-f139-12b09b475033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:15:03 - INFO - root -    Starting training / evaluation\n",
            "11/19/2023 05:15:03 - INFO - root -    Downloading Data Files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/Goutami-Sooda/ITRL-Project/main/Dataset(JSON).json\n",
            "139556/139556 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:15:03 - INFO - root -    Loading Data Files\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29a134935e5843df87bed26b2adc16e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc16b8c2b7cf46eab086418c50db4e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6a600d47f084d2d8beefb6749dc8269"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:15:03 - INFO - root -    Initializing Tokenizer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a781f68e8234babb9423cda4b5a4e89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c39ad8f25044fc4b44a2f17423516c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaf6bf40c9204486a56748685882cd6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fab71ebe959745b2b64da2ad444229a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a02d7710b3f6408d9bdd4010e37a2900"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:15:05 - INFO - root -    Preparing Features\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1123 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8486958ada0442e2850f1ae3673f88c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d60a1ed2f9a942179064a1018ff7c1f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:15:07 - INFO - root -    Intializing training and validation dataset \n",
            "11/19/2023 05:15:09 - INFO - root -    Intializing model | T5 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1001db3b11944d18af0d4508e12f51e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c303181723540b58d66312e76a92fb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:15:46 - INFO - root -   ***** Running training *****\n",
            "11/19/2023 05:15:46 - INFO - root -     Num examples = 1123\n",
            "11/19/2023 05:15:46 - INFO - root -     Num Epochs = 20\n",
            "11/19/2023 05:15:46 - INFO - root -     Total train batch size (w. parallel & distributed) = 8\n",
            "11/19/2023 05:15:46 - INFO - root -     Steps per epoch = 140\n",
            "11/19/2023 05:15:46 - INFO - root -     Total optimization steps = 2800\n",
            "11/19/2023 05:15:46 - INFO - root -   Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 794.8ms/step  training_loss: 0.0231 - learning_rate: 0.00007500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 719.3ms/step  eval_loss: 0.0037 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:18:01 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-1\n",
            "11/19/2023 05:18:01 - INFO - root -   Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.3ms/step  training_loss: 0.0035 - learning_rate: 0.00015000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 118.3ms/step  eval_loss: 0.0021 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:19:06 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-2\n",
            "11/19/2023 05:19:06 - INFO - root -   Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 357.5ms/step  training_loss: 0.0021 - learning_rate: 0.00022500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 115.7ms/step  eval_loss: 0.0022 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:20:10 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-3\n",
            "11/19/2023 05:20:10 - INFO - root -   Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 361.9ms/step  training_loss: 0.0021 - learning_rate: 0.00030000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 118.5ms/step  eval_loss: 0.0014 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:21:18 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-4\n",
            "11/19/2023 05:21:18 - INFO - root -   Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 357.7ms/step  training_loss: 0.0021 - learning_rate: 0.00028125 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 117.5ms/step  eval_loss: 0.0016 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:22:50 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-5\n",
            "11/19/2023 05:22:50 - INFO - root -   Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 357.1ms/step  training_loss: 0.0013 - learning_rate: 0.00026250 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 116.1ms/step  eval_loss: 0.0018 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:24:11 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-6\n",
            "11/19/2023 05:24:11 - INFO - root -   Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 359.0ms/step  training_loss: 0.0010 - learning_rate: 0.00024375 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 114.7ms/step  eval_loss: 0.0010 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:25:57 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-7\n",
            "11/19/2023 05:26:33 - INFO - root -   Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 359.7ms/step  training_loss: 0.0006 - learning_rate: 0.00022500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 114.0ms/step  eval_loss: 0.0005 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:28:01 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-8\n",
            "11/19/2023 05:28:01 - INFO - root -   Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.8ms/step  training_loss: 0.0004 - learning_rate: 0.00020625 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 123.0ms/step  eval_loss: 0.0005 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:29:30 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-9\n",
            "11/19/2023 05:30:23 - INFO - root -   Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.8ms/step  training_loss: 0.0003 - learning_rate: 0.00018750 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 118.3ms/step  eval_loss: 0.0004 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:31:59 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-10\n",
            "11/19/2023 05:32:45 - INFO - root -   Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 357.4ms/step  training_loss: 0.0002 - learning_rate: 0.00016875 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 113.3ms/step  eval_loss: 0.0004 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:34:23 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-11\n",
            "11/19/2023 05:35:07 - INFO - root -   Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.8ms/step  training_loss: 0.0002 - learning_rate: 0.00015000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 114.7ms/step  eval_loss: 0.0003 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:36:41 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-12\n",
            "11/19/2023 05:36:41 - INFO - root -   Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.8ms/step  training_loss: 0.0002 - learning_rate: 0.00013125 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 118.0ms/step  eval_loss: 0.0002 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:38:22 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-13\n",
            "11/19/2023 05:39:03 - INFO - root -   Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.7ms/step  training_loss: 0.0002 - learning_rate: 0.00011250 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 115.5ms/step  eval_loss: 0.0003 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:40:30 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-14\n",
            "11/19/2023 05:40:30 - INFO - root -   Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.9ms/step  training_loss: 0.0001 - learning_rate: 0.00009375 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 114.1ms/step  eval_loss: 0.0003 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:41:50 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-15\n",
            "11/19/2023 05:41:50 - INFO - root -   Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 357.0ms/step  training_loss: 0.0001 - learning_rate: 0.00007500 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 118.3ms/step  eval_loss: 0.0002 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:43:21 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-16\n",
            "11/19/2023 05:43:21 - INFO - root -   Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.1ms/step  training_loss: 0.0001 - learning_rate: 0.00005625 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 120.5ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:44:41 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-17\n",
            "11/19/2023 05:44:43 - INFO - root -   Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.9ms/step  training_loss: 0.0001 - learning_rate: 0.00003750 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 120.3ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:46:12 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-18\n",
            "11/19/2023 05:47:05 - INFO - root -   Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.1ms/step  training_loss: 0.0001 - learning_rate: 0.00001875 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 121.4ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:48:46 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-19\n",
            "11/19/2023 05:49:27 - INFO - root -   Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 140/140 [==============================] 356.7ms/step  training_loss: 0.0001 - learning_rate: 0.00000000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 16/16 [==============================] 123.4ms/step  eval_loss: 0.0001 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/19/2023 05:50:59 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-20\n",
            "11/19/2023 05:50:59 - INFO - root -   Training took: 0:35:13.181077\n",
            "11/19/2023 05:50:59 - INFO - root -    Saving model in runs//saved_model/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tf_model.h5:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0868feda852a4d31b128188eee7b839f"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}